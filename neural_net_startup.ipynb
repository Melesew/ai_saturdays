{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teaching a 3 level neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy for maths, pandas for reading data from text\n",
    "\n",
    "from numpy import random, exp, array, dot\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network class definition\n",
    "\n",
    "class Neural_Network():\n",
    "    def __init__(self, get_input, get_output):\n",
    "        # seed the random generator for easier debugging\n",
    "        random.seed(1)\n",
    "        \n",
    "        #Save all variables in self for future reference\n",
    "        self.gateInput = gateInput\n",
    "        self.gateOutput = gateOutput\n",
    "        self.input_shape = (1,3)\n",
    "        self.output_shape = (1,2)\n",
    "        self.layer_1_nodes = 5\n",
    "        self.layer_2_nodes = 5\n",
    "        self.layer_3_nodes = 5\n",
    "        \n",
    "         # Generate weights with value between -1 to 1 so that mean is overall 0\n",
    "        self.weights_1 = 2 * random.random((self.input_shape[1], self.layer_1_nodes)) - 1\n",
    "        self.weights_2 = 2 * random.random((self.layer_1_nodes, self.layer_2_nodes)) - 1\n",
    "        self.weights_3 = 2 * random.random((self.layer_2_nodes, self.layer_3_nodes)) - 1\n",
    "        self.out_weights = 2 * random.random((self.layer_3_nodes, self.output_shape[1])) - 1\n",
    "    \n",
    "    # Sigmoid function gives a value between 0 and 1\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + exp(-x))\n",
    "\n",
    "    # Reversed Sigmoid by derivating the value\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def relu(self, z):\n",
    "        A = np.maximum(0, z)\n",
    "\n",
    "        assert (A.shape == z.shape)\n",
    "        \n",
    "        return A\n",
    "\n",
    "\n",
    "    def relu_backward(self, dA, cache):\n",
    "        Z = cache\n",
    "        dZ = np.array(dA, copy=True)  # just converting dz to a correct object.\n",
    "\n",
    "        # When z <= 0, you should set dz to 0 as well.\n",
    "        dZ[Z <= 0] = 0\n",
    "\n",
    "        assert (dZ.shape == Z.shape)\n",
    "        return dZ\n",
    "    \n",
    "    def think(self, x):\n",
    "        # Multiply the input with weights and find its sigmoid activation for all layers\n",
    "        layer1 = self.sigmoid(dot(x, self.weights_1))\n",
    "        layer2 = self.sigmoid(dot(layer1, self.weights_2))\n",
    "        layer3 = self.sigmoid(dot(layer2, self.weights_3))\n",
    "        output = self.sigmoid(dot(layer3, self.out_weights))\n",
    "        \n",
    "        return output    \n",
    "    \n",
    "    def train(self, num_steps):\n",
    "        for x in range(num_steps):\n",
    "          # Same as code of thinking\n",
    "          layer1 = self.sigmoid(dot(self.gateInput, self.weights_1))\n",
    "          layer2 = self.sigmoid(dot(layer1, self.weights_2))\n",
    "          layer3 = self.sigmoid(dot(layer2, self.weights_3))\n",
    "          output = self.sigmoid(dot(layer3, self.out_weights))\n",
    "\n",
    "          # What is the error?\n",
    "          outputError = self.gateOutput - output\n",
    "\n",
    "          # Find delta, i.e. Product of Error and derivative of next layer\n",
    "          delta = outputError * self.sigmoid_derivative(output)\n",
    "\n",
    "          # Multiply with transpose of last layer\n",
    "          # to invert the multiplication we did to get layer \n",
    "          out_weights_adjustment = dot(layer3.T, delta)\n",
    "\n",
    "          # Apply the out_weights_adjustment\n",
    "          self.out_weights += out_weights_adjustment\n",
    "\n",
    "          # Procedure stays same, but the error now is the product of current weight and\n",
    "          # Delta in next layer\n",
    "          delta = dot(delta, self.out_weights.T) * self.sigmoid_derivative(layer3)\n",
    "          weight_3_adjustment = dot(layer2.T, delta)\n",
    "          self.weights_3 += weight_3_adjustment\n",
    "\n",
    "          delta = dot(delta, self.weights_3.T) * self.sigmoid_derivative(layer2)\n",
    "          weight_2_adjustment = dot(layer1.T, delta)\n",
    "          self.weights_2 += weight_2_adjustment\n",
    "\n",
    "          delta = dot(delta, self.weights_2.T) * self.sigmoid_derivative(layer1)\n",
    "          weight_1_adjustment = dot(self.gateInput.T, delta)\n",
    "          self.weights_1 += weight_1_adjustment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00525989 0.99016656]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  file = pd.read_csv(\"neural_net_startup_dataset.txt\", delimiter=',')\n",
    "\n",
    "  dataset = file.values\n",
    "\n",
    "  gateInput = dataset[:,:3]\n",
    "  gateOutput = dataset[:,3:]\n",
    "\n",
    "  neural_network = Neural_Network(gateInput, gateOutput)\n",
    "\n",
    "  neural_network.train(6000)\n",
    "\n",
    "  # Should be 0 , 1\n",
    "  print(neural_network.think([[0,0,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
